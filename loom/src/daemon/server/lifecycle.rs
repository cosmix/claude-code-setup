//! Daemon server lifecycle methods: start, stop, run.

use super::super::protocol::{read_message, write_message, Request, Response};
use super::broadcast::{spawn_log_tailer, spawn_status_broadcaster};
use super::client::handle_client_connection;
use super::core::{DaemonServer, MAX_CONNECTIONS};
use super::orchestrator::spawn_orchestrator;
use anyhow::{bail, Context, Result};
use nix::unistd::{close, fork, pipe, setsid, ForkResult};
use std::fs::{self, File, Permissions};
use std::os::unix::fs::PermissionsExt;
use std::os::unix::io::AsRawFd;
use std::os::unix::net::{UnixListener, UnixStream};
use std::path::Path;
use std::sync::atomic::Ordering;
use std::sync::Arc;
use std::thread;
use std::time::Duration;

impl DaemonServer {
    /// Stop a running daemon by sending a stop request via socket.
    ///
    /// # Arguments
    /// * `work_dir` - The .work/ directory path
    ///
    /// # Returns
    /// `Ok(())` on success, error if daemon is not running or communication fails
    pub fn stop(work_dir: &Path) -> Result<()> {
        let socket_path = work_dir.join("orchestrator.sock");

        if !Self::is_running(work_dir) {
            bail!("Daemon is not running");
        }

        // Read auth token
        let token_path = work_dir.join("daemon.token");
        let auth_token = fs::read_to_string(&token_path)
            .context("Failed to read auth token")?
            .trim()
            .to_string();

        let mut stream =
            UnixStream::connect(&socket_path).context("Failed to connect to daemon socket")?;

        stream
            .set_read_timeout(Some(Duration::from_secs(5)))
            .context("Failed to set read timeout")?;

        write_message(&mut stream, &Request::Stop { auth_token })
            .context("Failed to send stop request")?;

        let response: Response = match read_message(&mut stream) {
            Ok(resp) => resp,
            Err(e) => {
                if let Some(io_err) = e.downcast_ref::<std::io::Error>() {
                    if io_err.kind() == std::io::ErrorKind::WouldBlock
                        || io_err.kind() == std::io::ErrorKind::TimedOut
                    {
                        bail!(
                            "Daemon did not respond within 5 seconds. \
                             It may be frozen. Try: kill $(cat {})",
                            work_dir.join("orchestrator.pid").display()
                        );
                    }
                }
                return Err(e).context("Failed to read stop response");
            }
        };

        match response {
            Response::Ok => Ok(()),
            Response::AuthenticationFailed => bail!("Authentication failed - invalid token"),
            Response::Error { message } => bail!("Daemon returned error: {message}"),
            _ => bail!("Unexpected response from daemon"),
        }
    }

    /// Start the daemon (daemonize process).
    ///
    /// # Returns
    /// `Ok(())` on success, error if daemon fails to start
    pub fn start(&self) -> Result<()> {
        // Remove stale socket if it exists (ignore NotFound to avoid TOCTOU race)
        if let Err(e) = fs::remove_file(&self.socket_path) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove stale socket file");
            }
        }

        // Create pipe for error propagation from grandchild to original parent
        let (read_fd, write_fd) = pipe().context("Failed to create pipe")?;

        // First fork - parent exits, child continues
        match unsafe { fork() }.context("First fork failed")? {
            ForkResult::Parent { .. } => {
                // Close write end in parent
                drop(write_fd);

                // Wait for signal from grandchild
                let mut buf = [0u8; 1];
                match nix::unistd::read(&read_fd, &mut buf) {
                    Ok(1) if buf[0] == 1 => std::process::exit(0), // Success signal received
                    Ok(0) => {
                        // EOF - grandchild failed before writing success signal
                        eprintln!("Daemon failed to start");
                        std::process::exit(1);
                    }
                    _ => {
                        // Read error or unexpected data
                        eprintln!("Daemon failed to start");
                        std::process::exit(1);
                    }
                }
            }
            ForkResult::Child => {
                // Close read end in child
                drop(read_fd);
                // Child continues with daemonization (write_fd will be passed to grandchild)
            }
        }

        // Create new session (detach from controlling terminal)
        setsid().context("setsid failed")?;

        // Second fork - prevents acquiring a controlling terminal
        match unsafe { fork() }.context("Second fork failed")? {
            ForkResult::Parent { .. } => {
                // Intermediate parent exits
                std::process::exit(0);
            }
            ForkResult::Child => {
                // Grandchild continues as daemon
            }
        }

        // Write PID file
        fs::write(&self.pid_path, format!("{}", std::process::id()))
            .context("Failed to write PID file")?;

        // Restrict PID file to owner-only access to prevent tampering
        fs::set_permissions(&self.pid_path, Permissions::from_mode(0o600))
            .context("Failed to set PID file permissions")?;

        // Generate auth token and write to file
        let token = uuid::Uuid::new_v4().to_string();
        let token_path = self.work_dir.join("daemon.token");
        fs::write(&token_path, &token).context("Failed to write token file")?;
        fs::set_permissions(&token_path, Permissions::from_mode(0o600))
            .context("Failed to set token file permissions")?;

        // Redirect stdout and stderr to log file
        let log_file = File::create(&self.log_path).context("Failed to create log file")?;

        // Close stdin and redirect stdout/stderr to log file
        close(0).ok();
        // SAFETY: Using libc::dup2 directly with raw fds to avoid ownership issues.
        // fds 1 and 2 are valid open descriptors in this double-forked daemon process.
        unsafe {
            libc::dup2(log_file.as_raw_fd(), 1);
            libc::dup2(log_file.as_raw_fd(), 2);
        }

        // Signal success to original parent AFTER all initialization succeeds
        let success_signal = [1u8];
        let _ = nix::unistd::write(&write_fd, &success_signal);
        drop(write_fd); // Close pipe after writing success

        // Run the server
        self.run_server()
    }

    /// Run the daemon in foreground (for testing).
    ///
    /// # Returns
    /// `Ok(())` on success, error if server fails to start
    pub fn run_foreground(&self) -> Result<()> {
        // Write PID file manually
        fs::write(&self.pid_path, format!("{}", std::process::id()))
            .context("Failed to write PID file")?;

        // Restrict PID file to owner-only access to prevent tampering
        fs::set_permissions(&self.pid_path, Permissions::from_mode(0o600))
            .context("Failed to set PID file permissions")?;

        // Remove stale socket if it exists (ignore NotFound to avoid TOCTOU race)
        if let Err(e) = fs::remove_file(&self.socket_path) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove stale socket file");
            }
        }

        self.run_server()
    }

    /// Main server loop (listens on socket and accepts connections).
    pub(super) fn run_server(&self) -> Result<()> {
        // Set restrictive umask before socket bind to close TOCTOU window
        // between bind() and chmod(). The socket is created with permissions
        // determined by umask, so setting 0o077 ensures it's created as 0o600.
        let old_umask = unsafe { libc::umask(0o077) };
        let listener =
            UnixListener::bind(&self.socket_path).context("Failed to bind Unix socket")?;
        // Restore original umask immediately after bind
        unsafe {
            libc::umask(old_umask);
        }

        // Explicitly set permissions as defense-in-depth (umask should have handled this,
        // but being explicit is safer and documents intent)
        fs::set_permissions(&self.socket_path, Permissions::from_mode(0o600))
            .context("Failed to set socket permissions")?;

        // Set socket to non-blocking mode for graceful shutdown
        listener
            .set_nonblocking(true)
            .context("Failed to set socket to non-blocking")?;

        // Spawn the orchestrator thread to actually run stages
        let orchestrator_handle = spawn_orchestrator(self);

        // Spawn log tailing thread
        let log_tail_handle = spawn_log_tailer(self);

        // Spawn status broadcasting thread
        let status_broadcast_handle = spawn_status_broadcaster(self);

        while !self.shutdown_flag.load(Ordering::SeqCst) {
            match listener.accept() {
                Ok((stream, _addr)) => {
                    // Atomically increment connection count and check limit
                    let previous = self.connection_count.fetch_add(1, Ordering::SeqCst);
                    if previous >= MAX_CONNECTIONS {
                        // Over limit - decrement and reject
                        self.connection_count.fetch_sub(1, Ordering::SeqCst);
                        eprintln!("Connection limit reached ({MAX_CONNECTIONS}), rejecting");
                        drop(stream); // Close the connection immediately
                        continue;
                    }

                    let shutdown_flag = Arc::clone(&self.shutdown_flag);
                    let status_subscribers = Arc::clone(&self.status_subscribers);
                    let log_subscribers = Arc::clone(&self.log_subscribers);
                    let connection_count = Arc::clone(&self.connection_count);
                    let work_dir = self.work_dir.clone();

                    thread::spawn(move || {
                        let result = handle_client_connection(
                            stream,
                            shutdown_flag,
                            status_subscribers,
                            log_subscribers,
                            &work_dir,
                        );
                        // Decrement connection count when thread exits
                        connection_count.fetch_sub(1, Ordering::SeqCst);
                        if let Err(e) = result {
                            eprintln!("Client handler error: {e}");
                        }
                    });
                }
                Err(ref e) if e.kind() == std::io::ErrorKind::WouldBlock => {
                    // No connection available, sleep briefly but check shutdown frequently
                    thread::sleep(Duration::from_millis(10));
                }
                Err(e) => {
                    eprintln!("Accept error: {e}");
                    break;
                }
            }
        }

        // Wait for threads to finish with timeout (5 seconds)
        let join_timeout = Duration::from_secs(5);
        let join_check_interval = Duration::from_millis(50);

        // Helper closure to wait for a thread with timeout
        let wait_with_timeout = |handle: thread::JoinHandle<()>, name: &str| {
            let start = std::time::Instant::now();
            while !handle.is_finished() && start.elapsed() < join_timeout {
                thread::sleep(join_check_interval);
            }
            if handle.is_finished() {
                let _ = handle.join();
            } else {
                eprintln!("Warning: {} thread did not terminate within timeout", name);
                // Thread will be abandoned but the process is exiting anyway
            }
        };

        if let Some(handle) = orchestrator_handle {
            wait_with_timeout(handle, "orchestrator");
        }
        if let Some(handle) = log_tail_handle {
            wait_with_timeout(handle, "log_tail");
        }
        wait_with_timeout(status_broadcast_handle, "status_broadcast");

        self.cleanup()?;
        Ok(())
    }

    /// Clean up socket, PID, token, and completion marker files.
    pub(super) fn cleanup(&self) -> Result<()> {
        // Remove files directly, ignoring NotFound to avoid TOCTOU race
        if let Err(e) = fs::remove_file(&self.socket_path) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove socket file");
            }
        }
        if let Err(e) = fs::remove_file(&self.pid_path) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove PID file");
            }
        }
        // Clean up token file
        let token_path = self.work_dir.join("daemon.token");
        if let Err(e) = fs::remove_file(&token_path) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove token file");
            }
        }
        // Clean up completion marker file
        let completion_marker = self.work_dir.join("orchestrator.complete");
        if let Err(e) = fs::remove_file(&completion_marker) {
            if e.kind() != std::io::ErrorKind::NotFound {
                return Err(e).context("Failed to remove completion marker file");
            }
        }
        Ok(())
    }
}

impl Drop for DaemonServer {
    fn drop(&mut self) {
        let _ = self.cleanup();
    }
}
